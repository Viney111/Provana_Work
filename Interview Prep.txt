--SQL Performance Optimization Technique

1). Use Indexing (where, orderby,JOIN) Avoid Table Scan whereever possible.
2). Optimize Joins (Indexing on Join columns, if possible columns should be integers instead of varchars)
3). Minimise Number of Select columns
4). Use UNION ALL Instead of UNION, where possible
5). Partitioning the table, on appropriate column (HASH, RANGE, ROUNDROBIN)


--Optimization in Spark
1). Persist or Cache the dataframe that is frequently used
2). Partitioning the dataframe, on appropriate column
3). Broadcast variable (read-only copy of small dataset across all worker nodes)
4). Use Parquet file (columnar storage)

--In Apache Spark, processing large datasets (100GB) that exceed the available memory capacity (50GB) THEN Spark would handle it in following way:-
1). Data Partitioning (Small chunks of overall dataset, independent parallel processing)
2). Processing in Stages; load one or more partition in memory, performs transformations and then spills intermediate results in disk, if needed.
3). Disk Spill; if in memory space is less, it would use disk storage for read back
4). Shuffling:- groupbykey, reducebykey; writing intermediate data to disk for a short period during the shuffle phase.


--Significance of _success file in spark
Job Completion Indicator;Data Consistency Guarantee;Workflow Coordination (could be used for further dependent processes on ETL pipelines)


--Spark Architecture
Driver Program: 	Main Entry Point contains SparkContext 
Spark Context: 		responsible for coordinating tasks across cluster for resource allocation,represents connection to spark cluster
Cluster Manager: 	manage resource across cluster e.g. Hadoop YARN; tasks scheduling
Worker Node: 		Individual machines on cluster; performs actual data processing tasks; Each Worker node runs executor process
Executor: 		Processes runnig on worker nodes, Run multiple tasks in parallel and communicate with the driver program and Cluster Manager; Cache data in memory or disc
Task: 			Smallest unit in Spark; Created by SparkContext, executed on worker nodes
RDD: 			Fundamental DS in Spark
DAG: 			Sequence of Stages and tasks organised in DAG, perform lazy evaluation, only computing results when an action is triggered.
DataFrame & Dataset API:higher-level abstractions built on top of RDDs.

--How to handle bad data in Pyspark/Databricks
Default mode is spark.read.option("mode","PERMISSIVE"); It would read all the bad data in dataset
we could change the MODE = "FAILFAST" to raise exception or failure to identify that it contains bad data
MODE = "DROPMALFORMED" could reject the bad data altogether from the Spark dataframe
option = "badRecordsPath","badrecordsfolderpath" would separate the bad records in this folder in DATABRICKS to help us to backtrack which was the bad data
columnNameOfCorruptRecord="baddata"; by adding one more column in dataframe; and we could extract good data by filter where baddata column is NULL


--DWH vs DataLake vs Deltalake
DWH supports structured data only (not semi or unstructured), traditional, easier for DML operation, scaling up and down is not easy, costlier
Datalake supports all three types of data, less costly, store huge amount of data but does not support fully DML; if some process fails then corruped state in system as NO ACID Property
DeltaLake overcome disadvantges of DWH and Datalake; supports all type of data, ACID Property; easier to perform DML operations.(Reliability, Security and Performance)


--Fundamental Architecture of DeltaLake
1). Json Transaction Log Files (for every transaction) could do "TIME TRAVEL" with these files; 
2). Parquet checkpointing File (for every 10 json log files); traverse through 1000 (lots of) json transaction files made easy with this checkpointing File (captures current status of file)
3). CRC (Cyclic Redundant Check) to prevent accidental damage to data for Delta tables.

Delta lake follows "SOFT DELETE" approach (not deleted immidiately, default delete duration is after 7 days, could be changed as per use case); data is still there after delete query but would not show up in tables.
Delta table instance to be created out of Delta table only, to apply transaformations through Pyspark instead of SQL query


--Left Anti Join (to fetch all rows from left table except matched columns from right tables)


--ADF, Synaspe Related questions

mssparkutils.notebook.run(NotebookName,1800) --Calling Notebook by this library by passing notebook name.

Schedule vs Tumbling Window Trigger
1). Tumbling window can work for past (back dates) as well while schedule can't; In Tumbling window trigger, i could get start or end time of Tumbling trigger in system variable.
2). Dependency can be there in Tumbling
3). Concurrency 
4). One Pipeline:- One Tumbling Window Trigger ; many to many in Schedule Trigger

--To cancel the pipeline in the forloop if one activity fails,
while all processes are running Sequentially, we have to use "WEB Activity" (by posting) by restapi for cancel,to cancel that pipeline


--ADLS Gen-2 questions and answers
ACLs define permissions on directories and files within ADLS Gen2.

--SQL Keys
Primary Key - No NULL Value
Forgein Key - References to other table's primary key
Unique key - Same as Primary Key but does not allow NULL Values
Candidate Key - Set of one or more column which could become primary key



Map- Applies funtion to each single element of rdd
flatmap- Similar to map, but each input item can be mapped to zero or more output items
reduce- aggregate the elements of RDD to one value



